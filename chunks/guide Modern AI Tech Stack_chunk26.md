---
source: guide Modern AI Tech Stack.md
type: guide
chunk: 26
total_chunks: 37
---

### Prompt Evaluation and Optimization:

* **Promptfoo:** Enables structured testing and benchmarking of different prompts or models. This allows systematic improvements to both model performance and user experience over time.
* **Agenta:** Open-source LLM developer platform that has end-to-end tools for the entire LLMOps workflow. Simplifies the deployment and testing of LLM-powered applications by providing a collaborative platform for prompt engineering, model evaluation, and A/B testing of different prompts and configurations in production.
* **DSPy:** A user-friendly framework that helps you build AI systems without writing complex prompts. It automatically improves the prompts by learning what works best using evals, letting you focus on creating modular AI applications using simple Python code rather than struggling with prompt engineering.